%--------1---------2---------3---------4---------5---------6---------7
%
% Competition paper
% SBST 2019
% Page limit: 4 pages
%

%\documentclass[10pt, conference, compsocconf]{IEEEtran}
%\documentclass[10pt,conference,compsocconf]{IEEEtran}
%\documentclass[10pt,conference]{IEEEtran} 
\documentclass[sigconf,table]{acmart}

%\documentclass[times, 10pt,twocolumn]{article} 
%\usepackage{latex8}
%\usepackage{times}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage[english]{babel}
\usepackage{listings}
\usepackage{algorithmic}
\usepackage{float}
%\usepackage[numbers,sort&compress,square]{natbib}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{subfigure}
%\usepackage{hyperref}
\usepackage{color}
%\usepackage[usenames,dvipsnames,]{xcolor}
%\usepackage{soul}
\usepackage{xspace}
\usepackage{boxedminipage}
\usepackage{alltt}
\usepackage{multirow}
\usepackage{paralist}
\usepackage{amsmath}
\usepackage{balance}
\definecolor{light-gray}{gray}{0.90}

\floatstyle{ruled}
\newfloat{algorithm}{tbp}{loa}
\floatname{algorithm}{Algorithm}

\newtheorem{definition}{Definition}


 %krams hinter fontadjust ist neu
  \definecolor{lightgrey}{rgb}{0.90,0.90,0.90}
\lstset{escapeinside={(*}{*)}}
  \lstloadlanguages{java}
 \lstdefinelanguage{pseudocode}
  {morekeywords={if, else, initialize, return, for, each, in, global, new}
   }
  \lstset{
    tabsize=2,
    mathescape=true,
    escapeinside={(*}{*)},
    captionpos=t,
    framerule=0pt,
    backgroundcolor=\color{lightgrey},
    basicstyle=\scriptsize\ttfamily,
    keywordstyle=\footnotesize\bfseries,
    numbers=none,
    numberstyle=\tiny,
    numbersep=1pt,
    fontadjust,
    breaklines=true,
    breakatwhitespace=false
  }    
      

% \hypersetup{
% colorlinks=true,
% urlcolor=rltblue,
% linkcolor=rltred,
% citecolor=rltgreen,
% bookmarksnumbered=true,
% pdftitle={EvoSuite at the SBST 2016 Tool Competition},
% pdfauthor={Gordon Fraser and Andrea Arcuri},
% pdfsubject={Test case generation},
% pdfkeywords={Test case generation, unit testing, test
%   oracles, assertions, search based testing}
% }

\definecolor{rltred}{rgb}{0.5,0,0}
\definecolor{rltgreen}{rgb}{0,0.5,0}
\definecolor{rltblue}{rgb}{0,0,0.5}
\definecolor{ScarletRed}{rgb}{0.80,0.00,0.00}

\setcopyright{rightsretained}

% in draft mode we put \remarks into the margins and do other stuff
% set to \draftfalse for 
\newif\ifdraft
\draftfalse
\drafttrue

\ifdraft
	\marginparwidth=1.3cm
	\marginparsep=5pt
	\newcommand\remark[1]{%
		\mymarginpar{\raggedright\hbadness=10000\tiny\it #1\par}}
	% TODO marker
	\newcommand{\TODO}[1]{\textbf{\textcolor{ScarletRed}{[TODO: #1]}}\xspace}
\else
	\newcommand\remark[1]	{}
	\newcommand{\TODO}[1]{}
\fi

\ifdraft
	\overfullrule3pt
\fi    

% We use \FIXME for located problems (``defect'')
\newcommand{\FIXME}[1]{\remark{FIXME: #1}}
\newcommand\parremark[1]	{\par\textbf{REMARK:} #1\par}

\newcommand{\gordon}[1]{\textcolor{blue}{\sf\small\textbf{Gordon:} #1}}
\newcommand{\andrea}[1]{\textcolor{ScarletRed}{\sf\small\textbf{Andrea:} #1}}

% \mathid is used to denote identifiers and slots in formulas
\newcommand{\mathid}[1]{\text{\rmfamily\textit{#1}}}

% But usually, we shall use \|name| instead.
\def\|#1|{\mathid{#1}}

% \codeid is used to denote computer code identifiers
\newcommand{\codeid}[1]{\texttt{#1}}

% But usually, we shall use \<name> instead.
\def\<#1>{\codeid{#1}}

% Our results
\newenvironment{result}%
{\smallskip
\noindent
\let\emph=\textbf
\begin{boxedminipage}{\columnwidth}\begin{center}\em}%
{\end{center}\end{boxedminipage}%
\smallskip
}

\newcommand{\JodaTime}{Joda-Time\xspace}  % That's how they write themselves -- AZ

\newcommand{\EVOSUITE}{{\sc EvoSuite}\xspace}
\newcommand{\JTEXPERT}{{\sc jTExpert}\xspace}
\newcommand{\RANDOOP}{{\sc Randoop}\xspace}
\newcommand{\TT}{{\sc T3}\xspace}
\newcommand{\SUSHI}{{\sc Sushi}\xspace}
\newcommand{\TARDIS}{{\sc Tardis}\xspace}

\newcommand{\MUTEST}{{\sc $\mu$Test}\xspace}
\newcommand{\CS}{{\sc SF100}\xspace}

\newcommand{\TOTALPOINTS}{{255.43}\xspace}

\DeclareMathSymbol{,}{\mathpunct}{letters}{"3B}
\DeclareMathSymbol{,}{\mathord}{letters}{"3B}
\DeclareMathSymbol{\decimal}{\mathord}{letters}{"3A}
%%%"

\input{macros}

\newcommand{\NumCUTS}{{38}\xspace} 

%------------------------------------------------------------------------- g
% take the % away on next line to produce the final camera-ready version 
%\pagestyle{empty}

%------------------------------------------------------------------------- 
\begin{document}

\copyrightyear{2019} 
\acmYear{2019} 
\setcopyright{acmcopyright}
\acmConference[SBST'19]{SBST'19: IEEE/ACM 12th International Workshop on Search-Based Software Testing}{May 27, 2019}{Montreal, Canada}
\acmBooktitle{SBST'19: SBST'19: IEEE/ACM 12th International Workshop on Search-Based Software Testing, May 27, 2019, Montreal, Canada}
\acmPrice{15.00}
\acmDOI{}
\acmISBN{}

% 

%\title{Unit Testing Tool competition: Results for EvoSuite}
\title{\EVOSUITE at the SBST 2019 Tool Competition}
 
\author{Annibale Panichella}
\affiliation{%
  \institution{Delft University of Technology}
  \city{Delft}
  \country{Netherlands}
}
\email{a.panichella@tudelft.nl}

\author{Jos\'e Campos}
\affiliation{%
  \institution{University of Washington}
  \city{Seattle}
  \country{USA}
}
\email{jmcampos@uw.edu}

\author{Gordon Fraser}
\affiliation{%
  \institution{Chair of Software Engineering II, University of Passau}
  \city{Passau}
  \country{Germany}
}
\email{gordon.fraser@uni-passau.de}

% \affil\IEEEauthorrefmark{1}, \IEEEauthorrefmark{2}, Jos\'e Campos\IEEEauthorrefmark{3}}
%   \IEEEauthorblockA{The University of Sheffield\\Sheffield, United Kingdom\\
%     \{\IEEEauthorrefmark{1}gordon.fraser,
%     \IEEEauthorrefmark{2}j.rojas, \IEEEauthorrefmark{3}jose.campos\}@sheffield.ac.uk} 
% \and
% \IEEEauthorblockN{Andrea Arcuri}
%   \IEEEauthorblockA{Westerdals Oslo ACT, Norway\\and University of Luxembourg, Luxembourg\\arcand@westerdals.no}
% }

% The default list of authors is too long for headers.
\renewcommand{\shortauthors}{B. Trovato et al.}


\begin{abstract}
  \EVOSUITE is a search-based tool that automatically generates
  executable unit tests for Java code (JUnit tests).  This paper
  summarises the results and experiences of \EVOSUITE's participation
  at the seventh unit testing competition at SBST 2019, where \EVOSUITE
  achieved the highest overall score (\TOTALPOINTS points) for the
  sixth time in seven editions of the competition.
\end{abstract}

\maketitle
%\thispagestyle{empty}

%------------------------------------------------------------------------- 
\section{Introduction}
The annual unit test generation competition aims to drive and evaluate
progress on unit test generation tools. In the 7th instance of the
competition at the International Workshop on Search-Based Software
Testing (SBST) 2019, several JUnit test generation tools
%two tools, \EVOSUITE and \JTEXPERT\TODO{confirm},
competed on a set of \NumCUTS open-source Java classes. %, and 
% Two other tools,
%\RANDOOP and \TT, and 
%existing open-source test suites were used as
%baseline.
This paper describes the results obtained by the \EVOSUITE
test generation tool~\cite{FrA11c} in this competition. Details about
the procedure of the competition, the technical framework, and the
benchmark classes can be found in~\cite{sbst19competition}.  In this
competition, \EVOSUITE achieved an overall score of \TOTALPOINTS,
which was the highest among the competing and baseline
tools.

%------------------------------------------------------------------------- 
\section{About EvoSuite}


\begin{table}[!h]
\renewcommand{\arraystretch}{1.3}
\caption{Classification of the \EVOSUITE unit test generation
  tool}\label{tool-description}
\resizebox{1.0\columnwidth}{!}{  
\begin{tabular}{|l|p{5cm}|}
  \hline
  \multicolumn{2}{|l|}{Prerequisites} \\
  \hline
  Static or dynamic &  Dynamic testing at the Java class level\\
  Software Type &  Java classes\\
  Lifecycle phase&  Unit testing for Java programs\\
  Environment&  All Java development environments \\
  Knowledge required & JUnit unit testing for Java\\
  Experience required &  Basic unit testing knowledge\\
 \hline
  \multicolumn{2}{|l|}{Input and Output of the tool} \\
  \hline
 Input & Bytecode of the target class and dependencies \\
\hline
Output&  JUnit 4 test cases\\
 
  \hline
  \multicolumn{2}{|l|}{Operation} \\
  \hline
  Interaction &  Through the command line, and plugins for IntelliJ, Maven and Eclipse\\
  User guidance &  Manual verification of assertions for functional faults\\
  Source of information &  http://www.evosuite.org \\
  Maturity&  Mature research prototype, under development\\
  Technology behind the tool & Search-based testing / whole test suite generation \TODO{not whole test suite anymore}\\
\hline
  \multicolumn{2}{|l|}{Obtaining the tool and information} \\
  \hline
License & Lesser GPL V.3\\
Cost & Open source\\
Support & None \\
\hline
\hline
  \multicolumn{2}{|l|}{Does there exist empirical evidence about} \\
  \hline
  Effectiveness and Scalability & See~\cite{GoA_TSE12,fraser2014large} \\
%Completeness & \\
%Effectiveness & \\
%Efficiency & \\
%Defect types & \\
%Scalability & \\
%Comprehensibility & \\
%Learnability & \\
%Subjective satisfaction & \\
%Other & \\
\hline
\end{tabular}\vspace{-1em}
}
\end{table}


\EVOSUITE~\cite{FrA11c} is a search-based tool~\cite{GoA_TSE12} that
uses a genetic algorithm to automatically generate test suites for
Java classes. Given the name of a target class and the full Java
classpath (i.e., where to find the compiled bytecode of the class
under test and all its dependencies), \EVOSUITE automatically produces
a set of JUnit test cases aimed at maximising code coverage. \EVOSUITE
can be used on the command line, or through plugins for popular
development tools such as IntelliJ, Eclipse, or
Maven~\cite{ICST16_Tool}.

For this edition of the competition, \EVOSUITE was configured with \textit{Dynamic Many-Objective Sorting Algorithm} (DynaMOSA) as a search algorithm. DynMOSA~\cite{dynamosa, panichella:ssbse2018} is a recently developed many-objective algorithm that incrementally optimizes multiple coverage criteria at the same time. More specifically, different testing coverage requirements (e.g., branches) are treated as distinct, contrasting search objectives in a many-objective paradigm. Search objectives are computed using standard heuristics for code coverage, such as the branch distance and the approach level (see~\cite{GoA_TSE12} for more details). Coverage requirements are prioritized during the search according to their structural dependencies in the control dependency graph. The search is initialized with the coverage requirements (e.g., branches) positioned higher in the hierarchy; then, the remaining requirements are incrementally inserted in later generations when their dominator requirements are covered.

DynaMOSA evolves test cases~\cite{dynamosa}, which correspond to the chromosomes for the search. Each test case consists of variable-length sequences of Java statements (e.g., primitive statements and calls on the class under test). A population of randomly generated test cases is evolved using search operators (e.g., selection, crossover and mutation) to iteratively produce test cases that are closer to satisfy the target coverage requirements. The final test suite is formed by the shorter test cases encountered during the search, and that satisfy the coverage requirements. These tests are stored in an external data structure, called \textit{archive}~\cite{mosa,dynamosa}.

\EVOSUITE can be configured to satisfy for multiple coverage criteria at the same time, and the default configuration combines branch coverage with
mutation testing~\cite{emse14_mutation} and other basic
criteria~\cite{rojas2015combining}. Once the search is completed, \EVOSUITE applies various post-search optimizations aimed to improve the readability of
the generated tests. For example, tests are minimized and test assertions are selected using mutation analysis~\cite{10.1109/TSE.2011.93}. For more details on post-process optimization we refer to~\cite{FrA11c,FrA13a}.


In the past, the effectiveness of \EVOSUITE has been evaluated on open
source as well as industrial software in terms of code
coverage~\cite{fraser2014large,emse_archive,ea_evaluation,dynamosa},
fault finding
effectiveness~\cite{shamshiri2015automatically,moein2017}, and effects
on developer productivity~\cite{TOSEM_userstudy,ISSTA15_Study} and
software maintenance~\cite{ICST2018_Maintenance}.
%
\EVOSUITE has a longstanding record of success at the unit testing
tool competition, having ranked second in the third edition of the
competition~\cite{evosuiteAtSbst2015} and first in all the other
editions~\cite{evosuiteAtSbst2013,evosuiteAtFittest2013,evosuiteAtSbst2016,evosuiteAtSbst2017,evosuiteAtSbst2018}.


%To improve performance further, we are investigating several
%extensions to \EVOSUITE. 
%For example, \EVOSUITE can employ dynamic
%symbolic execution~\cite{evoISSRE113} 
%and memetic algorithms~\cite{fraser2014memetic}
%to handle the cases in which our genetic algorithm may struggle. 


%------------------------------------------------------------------------- 
\section{Competition Setup}

The configuration of \EVOSUITE for the 2019 competition is largely
based on its default values, since these have been tuned
extensively~\cite{arcuri2013parameter}. We used the default set of
coverage criteria~\cite{rojas2015combining} (e.g., line coverage,
branch coverage, branch coverage by direct method invocation, weak
mutation testing, output coverage, exception coverage). \EVOSUITE uses
an archive of solutions~\cite{emse_archive} to keep the search focused
on uncovered goals, iteratively discarding covered goals and storing
the tests that covered them. After a certain percentage of the search
budget has passed and with a certain probability, \EVOSUITE starts
using mock objects (relying on Mockito) instead of actual class
instances~\cite{ICST_Mocking17} for dependencies; only branches that
cannot be covered without mocks lead to tests with mock objects. As in
the previous instance of the competition, we continue to use
frequency-based weighted constants for
seeding~\cite{sakti2015instance} and support Java Enterprise Edition
features~\cite{arcuri2016java}. %\TODO{any other noteworthy change?} No.
\EVOSUITE is actively maintained, therefore several bug fixes and
minor improvements have been applied since the last instance of the
competition, in particular in relation to non-determinism and flaky
tests~\cite{arcuri2014automated}, as well as new types of test
assertions (e.g., related to arrays and standard container classes).

Like in previous instances of the competition, we enabled the
post-processing step of test minimization---not for efficiency
reasons, but because minimized tests are less likely to break. To
reduce the overall time of test generation we included all assertions
rather than filtering them with mutation
analysis~\cite{10.1109/TSE.2011.93}, which is computationally
expensive. The use of all assertions has a negative impact on
readability, but this is not evaluated as part of the SBST contest.

Four time budgets were used to call each tool: 10, 60, 120 and 240
seconds. We used the same strategy used in previous competition
(e.g.,~\cite{evosuiteAtSbst2016}) to distribute the overall time
budget onto the different phases of \EVOSUITE (e.g., initialisation,
search, minimization, assertion generation, compilation check, removal
of flaky tests). That is, 50\% of the time was allocated to the
search, and the rest was distributed equally to the remaining phases.


%------------------------------------------------------------------------- 
\section{Benchmark Results}

\begin{table*}[t]
  \centering
  \caption{\label{table:results}Detailed results of \EVOSUITE on the
    SBST benchmark classes.}
\vspace{-1em}
\resizebox{\textwidth}{!}{  
\input{mainTable.tex}
}	
\end{table*}

%\begin{table*}[t]
%  \centering
%  \caption{\label{table:t3_results}Detailed results of \TT on the
%    SBST benchmark classes.}
%\vspace{-1em}
%\resizebox{\textwidth}{!}{  
%\input{mainTable-t3.tex}
%}	
%\end{table*}

% \begin{table*}[t]
%   \centering
%   \caption{\label{table:coverage_results}Detailed coverage results of \EVOSUITE on the SBST benchmark classes.}
% \resizebox{0.8\textwidth}{!}{  
% \input{coverageTable.tex}
% }
% \end{table*}

% \begin{table*}[t]
%   \centering
%   \caption{\label{table:fault_results}Detailed fault detection results of \EVOSUITE on the SBST benchmark classes.}
% \resizebox{0.8\textwidth}{!}{  
% \input{faultTable.tex}
% }	
% \end{table*}

%\paragraph{Overall Results}


%Table~\ref{table:results} lists the branch coverage and mutation
%analysis results achieved by \EVOSUITE on all benchmark classes in the
%contest. Coverage and mutation scores are generally in the expected
%ranges, with clear overall increases for higher time budgets. With the
%highest time budget of $240s$, the average branch coverage achieved
%was \AvgCovD, lower that last year's 64.6\% for the same time budget,
%but the average mutation score was \AvgMutD, higher than last year's
%49.6\% for the same time budget.

%\paragraph{Manually Written Tests}

%This year, the contest also included manually written test suites as
%baseline (for 49 out of 59 benchmarks). The results indicate that
%there is no significant difference in branch coverage between
%\EVOSUITE-generated test suites (avg. \AvgCovEvosuite) and manually
%written ones (avg. \AvgCovManual), with Vargha-Delaney's
%$A_{12}=\EvoManCovA$ and $p=\EvoManCovPV$. In terms of mutation
%scores, surprisingly, the results show that automatically generated
%assertions are competitive with manually written ones: the average
%\EVOSUITE mutation score for these 49 benchmarks is \AvgMutEvosuite
%and the average mutation score obtained by developer-written test
%suites is \AvgMutManual ($A_{12}=\EvoManMutA$ and
%$p=\EvoManMutPV$).

% Coverage on several
% of the benchmarks from Closure is low, which matches previous
% findings~\cite{shamshiri2015automatically}.

% Results in terms of mutation scores and fault detection ratio (i.e.,
% how many of the runs had at least one failing test on the
% corresponding bug) are showing in
% Table~\ref{table:fault_results}. Again the Closure results are
% generally worse than those of other projects, both in terms of
% mutation score and ratio of fault detection.


%~\EVOSUITE generated \FlakyEvosuite flaky tests per run on average,
%much lower than the number of flaky tests produced by the competing
%and baseline tools (\FlakyTthree by \TT, \FlakyJtexpert by \JTEXPERT,
%and \FlakyRandoop by \RANDOOP). We attribute these relatively positive
%results---also consistent with previous years' results---to the way
%\EVOSUITE handles execution environments during test generation (e.g.,
%controlling the static state of the class under test, mocking
%interactions with the file system, system calls like
%\texttt{System.in} and \texttt{System.currentTimeMillis},
%etc.)~\cite{arcuri2014automated}.  \EVOSUITE provides deterministic
%replacement classes for many classes with known non-determinism,
%(e.g., those related to date and time), and in particular the FASTJSON
%benchmarks made heavy use of these. For example, multiple calendar
%systems are necessary (e.g., classes \texttt{HijrahDate},
%\texttt{MinguoDate}, \texttt{JapaneseDate},
%\texttt{ThaiBuddhistDate}), and \EVOSUITE instantiated its own
%replacement versions of these classes 719 times altogether, thus
%potentially avoiding many flaky tests. Overall, no flaky tests were
%generated for the vast majority of subjects in the competition.

%Amongst those benchmarks for which
%\EVOSUITE struggled with flakiness, FASTJSON-4 is a notable example:
%in the worst case, 90 flaky tests were generated in one single run. An
%analysis of the execution logs indicates several possible sources for
%this flakiness: a) a non-final static constant that is not properly
%reset in between executions; b) a possibly incorrect handling of enum
%types; and c) a global property not being reset in between
%executions. Considering that there were still remaining flaky tests, further investigation will be needed to confirm these
%conjectures and act upon them.


%\paragraph{Mocking}

%As mentioned in the previous section, \EVOSUITE uses private API
%access and functional mocking% in addition to environment
%% mocking
%~\cite{ICST_Mocking17}. Overall, in all tests generated in this
%competition, 1,339 private fields were set and 2,409
%private methods were invoked using reflection, and 15,369 Mockito mock
%instances were created and configured using another 9,553 statements
%(e.g., using the \text{doReturn} method). Considering the 1,804,932 overall test
%statements, this means that only 1.4\% of the tests were devoted to
%mocking. Amongst the 331 distinct classes for which mocks were
%generated at least once there are some classes for which EvoSuite can
%instantiate regular objects (e.g., \texttt{List}). This suggests that
%EvoSuite sometimes struggles to generate and set up complex objects
%correctly, and there is potential for future improvements.

%%a total of 2,766 mock
%%instances were created, using mocked access. Some of the Java
%%classes most frequently used for mocking across benchmarks are
%%\texttt{File} (849 instances in total), \texttt{Throwable} (284),
%%\texttt{PrintStream} (210) and \texttt{Thread} (114). 

%% . Further
%% investigation would be needed to assess the impact of these mocking
%% features on test generation effectiveness for the benchmarks in the
%% competition.

On the whole, the performance of \EVOSUITE was in line with previous
results, although \EVOSUITE seemed to fail on a notably large number
of classes. In particular, \EVOSUITE failed to produce any test
suites for benchmarks DUBBO-2, SPOON-169, SPOON-25, and WEBMAGIC-4,
and generally struggled with most AUTHZFORCE benchmarks (highlighted
in grey in Table~\ref{table:results}).

A closer look at the benchmarks DUBBO-2 and WEBMAGIC-4 reveals that
there were missing dependencies in the competition
setup~\cite{sbst19competition} (similar to cases in previous
years~\cite{evosuiteAtSbst2018}), which explains why \EVOSUITE was
incapable of generating any test: When a dependency of the class
under test is missing, then \EVOSUITE intentionally aborts with an
error to inform the user of the configuration error. While this is
desirable behavior during regular usage, arguably in the scope of the
competition \EVOSUITE could try to ignore such errors and try to
generate tests nevertheless, as it might still be possible to cover
code even with some missing dependencies. For the benchmark
FASTJSON-3 and a 10s time budget, \EVOSUITE generated some test
suites but due to a bug in the
competition infrastructure, no data was collected. % FileNotFoundException in the infrastructure

For the benchmarks SPOON-169 and SPOON-25, \EVOSUITE either failed to generated
a compilable test suite or crashed due to a bug in DynaMOSA. Although \EVOSUITE's
minimization could help at reducing the number of uncompilable / flaky test cases,
for SPOON-169 all minimizations attempts ran out of time.
% SPOON-169
% Minimization timeout. Roll back to original test suite
% and therefore the test suite 

For the AUTHZFORCE benchmarks, although \EVOSUITE managed to generate
some test suites, most executions for data collection ended with a
failure. Specifically, for these classes an exception was raised for
test execution for \emph{all} tools in the competition. However,
while this exception was not problematic for other tools, it caused
an inconsistent state of the JVM which led to the \texttt{@Before}
setup methods of \EVOSUITE to fail (the \texttt{@Before} method was
not able to configure \EVOSUITE's custom security manager because it
was never correctly removed). As a consequence, all test executions
failed. We were not able to reproduce the error in the competition
infrastructure, and thus cannot conjecture about the causes. The same
problem also occurred for SPOON-253. While in principle it would be
possible to extend \EVOSUITE to try to ignore such errors and keep on
executing tests, as explained it is \EVOSUITE's philosophy that the
user should be made aware of configuration errors, and hence such an
extension would only serve to tune \EVOSUITE for the competition.

In general, \EVOSUITE often struggles with a time budget as low as 10
seconds. For example, Table~\ref{table:results} reports ``-'' for
benchmarks FASTJSON-1, FASTJSON-3, FESCAR-41, FESCAR-42, and
SPOON-105.
% last message in the log files was * Initial Number of Goals in DynMOSA = 942 / 10337
%\TODO{either infinite loop, took much time to start evolving, or DynaMOSA can't
%be used in this benchmarks with a time budget of 10s.}
%
Surprisingly, for benchmarks SPOON-155, SPOON-16, SPOON-20, and
SPOON-211, \EVOSUITE successfully generated test suites for small
time budgets but it failed to do it for large time budgets. For all,
\EVOSUITE crashed due to a bug in the DynaMOSA algorithm which had
not been revealed by our previous experiments. Consequently, even
though DynaMOSA is known to increase the coverage \EVOSUITE achieves
subtantially in general, this increase was countered with some
missing datapoints. As always, the competition was helpful in
improving the robustness of the tool, by revealing this and other
bugs.
%% SPOON-155 (240s), SPOON-16 (120s), SPOON-20 (120s and 240s), SPOON-211 (120s)
%java.lang.NullPointerException: null
%	at org.evosuite.ga.operators.ranking.RankBasedPreferenceSorting.getNumberOfSubfronts(RankBasedPreferenceSorting.java:158) ~[evosuite-master-1.0.7-SNAPSHOT.jar:1.0.7-SNAPSHOT]
%	at org.evosuite.ga.metaheuristics.mosa.DynaMOSA.generateSolution(DynaMOSA.java:152) ~[evosuite-master-1.0.7-SNAPSHOT.jar:1.0.7-SNAPSHOT]
%	at org.evosuite.strategy.MOSuiteStrategy.generateTests(MOSuiteStrategy.java:115) ~[evosuite-master-1.0.7-SNAPSHOT.jar:1.0.7-SNAPSHOT]



%------------------------------------------------------------------------- 
\section{Conclusions}

This paper reports on the participation of the \EVOSUITE test
generation tool in the 7th SBST Java Unit Testing Tool Contest. With
an overall score of \TOTALPOINTS points, \EVOSUITE achieved the
highest score of all tools in the competition.


To learn more about \EVOSUITE, visit our Web site:
\begin{center}
%\url{http://evosuite.org/}
\texttt{http://www.evosuite.org}
\end{center}


%------------------------------------------------------------------------- 

%\noindent
\textbf{Acknowledgments:} Many thanks to all the contributors to
\EVOSUITE.  This project has been funded by the EPSRC project
``GREATEST'' (EP/N023978/2), and by the National Research Fund,
Luxembourg (FNR/P10/03).


%------------------------------------------------------------------------- 
%\def\IEEEbibitemsep{5pt plus 1pt}
%\def\IEEEbibitemsep{6pt}
%\clearpage
\bibliographystyle{IEEEtranS}
\bibliography{papers}
\balance

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
