%--------1---------2---------3---------4---------5---------6---------7
%
% Competition paper
% SBST 2013
% Page limit: 4 pages
%

%\documentclass[10pt, conference, compsocconf]{IEEEtran}
%\documentclass[10pt,conference,compsocconf]{IEEEtran}
\documentclass[10pt,conference]{IEEEtran} 


%\documentclass[times, 10pt,twocolumn]{article} 
%\usepackage{latex8}
%\usepackage{times}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage[english]{babel}
\usepackage{listings}
\usepackage{algorithmic}
\usepackage{float}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{subfigure}
%\usepackage{hyperref}
\usepackage{color}
\usepackage[usenames,dvipsnames,table]{xcolor}
%\usepackage{soul}
\usepackage{xspace}
\usepackage{boxedminipage}
\usepackage{alltt}
\usepackage{multirow}
\usepackage{paralist}
\usepackage{amsmath}
\usepackage{balance}


\floatstyle{ruled}
\newfloat{algorithm}{tbp}{loa}
\floatname{algorithm}{Algorithm}

\newtheorem{definition}{Definition}


 %krams hinter fontadjust ist neu
  \definecolor{lightgrey}{rgb}{0.90,0.90,0.90}
\lstset{escapeinside={(*}{*)}}
  \lstloadlanguages{java}
 \lstdefinelanguage{pseudocode}
  {morekeywords={if, else, initialize, return, for, each, in, global, new}
   }
  \lstset{
    tabsize=2,
    mathescape=true,
    escapeinside={(*}{*)},
    captionpos=t,
    framerule=0pt,
    backgroundcolor=\color{lightgrey},
    basicstyle=\scriptsize\ttfamily,
    keywordstyle=\footnotesize\bfseries,
    numbers=none,
    numberstyle=\tiny,
    numbersep=1pt,
    fontadjust,
    breaklines=true,
    breakatwhitespace=false
  }    
      

% \hypersetup{
% colorlinks=true,
% urlcolor=rltblue,
% linkcolor=rltred,
% citecolor=rltgreen,
% bookmarksnumbered=true,
% pdftitle={EvoSuite at the SBST 2016 Tool Competition},
% pdfauthor={Gordon Fraser and Andrea Arcuri},
% pdfsubject={Test case generation},
% pdfkeywords={Test case generation, unit testing, test
%   oracles, assertions, search based testing}
% }

\definecolor{rltred}{rgb}{0.5,0,0}
\definecolor{rltgreen}{rgb}{0,0.5,0}
\definecolor{rltblue}{rgb}{0,0,0.5}
\definecolor{ScarletRed}{rgb}{0.80,0.00,0.00}



% in draft mode we put \remarks into the margins and do other stuff
% set to \draftfalse for 
\newif\ifdraft
\draftfalse

\ifdraft
	\marginparwidth=1.3cm
	\marginparsep=5pt
	\newcommand\remark[1]{%
		\mymarginpar{\raggedright\hbadness=10000\tiny\it #1\par}}
\else
	\newcommand\remark[1]	{}
\fi

\ifdraft
	\overfullrule3pt
\fi    

% We use \FIXME for located problems (``defect'')
\newcommand{\FIXME}[1]{\remark{FIXME: #1}}
\newcommand\parremark[1]	{\par\textbf{REMARK:} #1\par}

\newcommand{\gordon}[1]{\textcolor{blue}{\sf\small\textbf{Gordon:} #1}}
\newcommand{\andrea}[1]{\textcolor{ScarletRed}{\sf\small\textbf{Andrea:} #1}}

% \mathid is used to denote identifiers and slots in formulas
\newcommand{\mathid}[1]{\text{\rmfamily\textit{#1}}}

% But usually, we shall use \|name| instead.
\def\|#1|{\mathid{#1}}

% \codeid is used to denote computer code identifiers
\newcommand{\codeid}[1]{\texttt{#1}}

% But usually, we shall use \<name> instead.
\def\<#1>{\codeid{#1}}

% Our results
\newenvironment{result}%
{\smallskip
\noindent
\let\emph=\textbf
\begin{boxedminipage}{\columnwidth}\begin{center}\em}%
{\end{center}\end{boxedminipage}%
\smallskip
}

\newcommand{\JodaTime}{Joda-Time\xspace}  % That's how they write themselves -- AZ

\newcommand{\EVOSUITE}{{\sc EvoSuite}\xspace}
\newcommand{\MUTEST}{{\sc $\mu$Test}\xspace}
\newcommand{\CS}{{\sc SF100}\xspace}

% TODO marker
\newcommand{\TODO}[1]{\sethlcolor{yellow}\textbf{\textcolor{ScarletRed}{\hl{TODO: #1}}}\xspace}


\DeclareMathSymbol{,}{\mathpunct}{letters}{"3B}
\DeclareMathSymbol{,}{\mathord}{letters}{"3B}
\DeclareMathSymbol{\decimal}{\mathord}{letters}{"3A}
%%%"

%\documentstyle[times,art10,twocolumn,latex8]{article}



%------------------------------------------------------------------------- 
% take the % away on next line to produce the final camera-ready version 
%\pagestyle{empty}

%------------------------------------------------------------------------- 
\begin{document}

% 

%\title{Unit Testing Tool competition: Results for EvoSuite}
\title{EvoSuite at the SBST 2017 Tool Competition}
 

\author{\IEEEauthorblockN{Authors}}


\maketitle
%\thispagestyle{empty}

\begin{abstract}
  \EVOSUITE is a search-based tool that automatically
  generates unit tests for Java code.  This paper summarizes the
  results and experiences of \EVOSUITE's participation at the fourth
  unit testing competition at SBST 2016, where \EVOSUITE achieved the
  highest overall score. 
\end{abstract}



%------------------------------------------------------------------------- 
\section{Introduction}

This paper describes the results of applying the \EVOSUITE test
generation tool~\cite{FrA11c} to the benchmark used in the tool
competition at the International Workshop on Search-Based Software
Testing (SBST) 2016.  Details about the competition and the benchmark
can be found in~\cite{sbst16competition}. 
In this competition, \EVOSUITE achieved a 1126.7 overall score, which was the highest among
the competing tools. 

%------------------------------------------------------------------------- 
\section{About EvoSuite}


\begin{table}[!h]
\renewcommand{\arraystretch}{1.3}
\caption{Classification of the \EVOSUITE unit test generation
  tool.}\label{tool-description}
\resizebox{1.0\columnwidth}{!}{  
\begin{tabular}{|l|p{5cm}|}
  \hline
  \multicolumn{2}{|l|}{Prerequisites} \\
  \hline
  Static or dynamic &  Dynamic testing at the Java class level\\
  Software Type &  Java classes\\
  Lifecycle phase&  Unit testing for Java programs\\
  Environment&  All Java development environments \\
  Knowledge required & JUnit unit testing for Java\\
  Experience required &  Basic unit testing knowledge\\
 \hline
  \multicolumn{2}{|l|}{Input and Output of the tool} \\
  \hline
 Input & Bytecode of the target class and dependencies \\
\hline
Output&  JUnit test cases (version 3 or 4)\\
 
  \hline
  \multicolumn{2}{|l|}{Operation} \\
  \hline
  Interaction &  Through the command line, and plugins for IntelliJ, Maven and Eclipse\\
  User guidance &  manual verification of assertions for functional faults\\
  Source of information &  http://www.evosuite.org \\
  Maturity&  Mature research prototype, under development\\
  Technology behind the tool & Search-based testing / whole test suite generation\\
\hline
  \multicolumn{2}{|l|}{Obtaining the tool and information} \\
  \hline
License & Lesser GPL V.3\\
Cost & Open source\\
Support & None \\
\hline
\hline
  \multicolumn{2}{|l|}{Does there exist empirical evidence about} \\
  \hline
  Effectiveness and Scalability & See~\cite{GoA_TSE12,fraser2014large}. \\
%Completeness & \\
%Effectiveness & \\
%Efficiency & \\
%Defect types & \\
%Scalability & \\
%Comprehensibility & \\
%Learnability & \\
%Subjective satisfaction & \\
%Other & \\
\hline
\end{tabular}\vspace{-1em}
}
\end{table}


\EVOSUITE~\cite{FrA11c,GoA_TSE12} automatically generates test suites
for Java classes, targeting branch coverage and other coverage
criteria (e.g., mutation testing~\cite{emse14_mutation}). \EVOSUITE
works at the Java bytecode level, i.e., it does not require source
code. It is fully automated and requires no manually written test
drivers or parameterized unit tests.  For example, when \EVOSUITE is
used from its Eclipse and IntelliJ plugins, a user
just needs to select a class, and tests are generated with a mouse-click.

\EVOSUITE has been evaluated on millions of lines of Java
code~\cite{fraser2014large}, both open-source code and close-source
code provided by one of our industrial partners.  In the first two
editions of the unit testing tool competition, \EVOSUITE ranked
first~\cite{evosuiteAtSbst2013,evosuiteAtFittest2013}, whereas it
ranked second in the third one.


\EVOSUITE uses an evolutionary approach to derive these test suites: A
genetic algorithm evolves candidate individuals (chromosomes) using
operators inspired by natural evolution (e.g., selection, crossover
and mutation), such that iteratively better solutions with respect to
the optimization target (e.g., branch coverage) are produced.  For
details on this test generation approach we refer to~\cite{GoA_TSE12}.
%To improve performance further, we are investigating several
%extensions to \EVOSUITE. 
%For example, \EVOSUITE can employ dynamic
%symbolic execution~\cite{evoISSRE113} 
%and memetic algorithms~\cite{fraser2014memetic}
%to handle the cases in which our genetic algorithm may struggle. 


As the generated unit tests are meant for human
consumption~\cite{fraser2013does}, \EVOSUITE applies various
post-processing steps to improve readability (e.g., minimising) and
adds test assertions that capture the current behavior of the tested
classes. To select the most effective assertions, \EVOSUITE uses
mutation analysis~\cite{10.1109/TSE.2011.93}.  \EVOSUITE can also be
used to automatically find faults such as undeclared thrown exceptions
and broken code contracts~\cite{emse13_oracle}.  For more details on
the tool and its abilities we refer to~\cite{FrA11c,FrA13a}.

%------------------------------------------------------------------------- 
\section{Competition Setup}

\EVOSUITE can be configured to target different coverage criteria. The
fitness function to drive the genetic algorithm was based on a
combination of several criteria~\cite{rojas2015combining} (e.g., line
coverage, branch coverage, branch coverage by direct method
invocation, weak mutation testing, output coverage, exception
coverage). \EVOSUITE now by default uses an archive of
solutions~\cite{emse_archive}, which means that throughout the search,
whenever a new coverage goal is satisfied, the corresponding test is
stored in the archive, and this goal is no longer targeted by the
fitness function. We enabled the post-processing step of test
minimization, but to reduce the time spent we included all assertions
rather than filtering them with mutation
analysis~\cite{10.1109/TSE.2011.93}. The use of all assertions has
effects on readability and the chances of obtaining flaky
tests. However, as readability is not measured by the SBST contest
metric, and many of the improvements to \EVOSUITE since the last
competition target flaky tests, we deemed this not a problem.

In contrast to previous instances of the competition, the test
generation tools this time received a time budget as input, and then
had to generate tests within that time. \EVOSUITE uses a combination
of different timeouts for its individual phases (e.g., initialization,
search, minimization, assertion generation, compilation check, removal
of flaky tests), which created the challenge of distributing the
overall budget onto these phases. We used a simple approach where 50\%
of the time was allocated to the search, whereas the other half of the
time was distributed equally to the remaining phases. If any of the
phases used more time than allocated, which can for example happen if
test executions take long or lead to timeouts, then phases for which
there is no time left are skipped. For example, if there is no time
left for minimization, then the raw test suite as generated by the
search is returned.

%------------------------------------------------------------------------- 
\section{Benchmark results}

% \begin{table*}[t]
%   \centering
%   \caption{\label{table:results}Detailed results of \EVOSUITE on the SBST benchmark classes.}
% \resizebox{0.9\textwidth}{!}{  
% \input{mainTable.tex}
% }	
% \end{table*}

\begin{table*}[t]
  \centering
  \caption{\label{table:coverage_results}Detailed coverage results of \EVOSUITE on the SBST benchmark classes.}
\resizebox{0.8\textwidth}{!}{  
%\input{coverageTable.tex}
}
\end{table*}

\begin{table*}[t]
  \centering
  \caption{\label{table:fault_results}Detailed fault detection results of \EVOSUITE on the SBST benchmark classes.}
\resizebox{0.8\textwidth}{!}{  
%\input{faultTable.tex}
}	
\end{table*}



The coverage results (line coverage and branch coverage) achieved by
\EVOSUITE on the benchmark classes are listed in
Table~\ref{table:coverage_results}. Coverage is generally in the
expected range, with clear increases for higher time budgets. Coverage
on several of the benchmarks from Closure is low, which matches
previous findings~\cite{shamshiri2015automatically}. 

Results in terms of mutation scores and fault detection ratio (i.e.,
how many of the runs had at least one failing test on the
corresponding bug) are showing in
Table~\ref{table:fault_results}. Again the Closure results are
generally worse than those of other projects, both in terms of
mutation score and ratio of fault detection.

On average, \EVOSUITE generated 0.14 flaky tests per run, a value that
is significantly lower than that of any of the competing tools. This
is due to extensive efforts to isolate \EVOSUITE tests from the
execution environment~\cite{arcuri2014automated}. A few of these flaky
tests were introduced by recent changes to \EVOSUITE following
experiments on Defects4J~\cite{shamshiri2015automatically}: \EVOSUITE
now includes assertions on the source of exceptions, similar to
commercial tools like Agitar One. %~\cite{agitarone}. 
Unfortunately, there were several instances in the competition where these assertions
lead to flaky tests. For example, the following is an excerpt from a
test for the Defects4J bug Lang-41, generated by \EVOSUITE:
\vspace{1em}

\begin{lstlisting}
@Test(timeout = 4000)
public void test19()  throws Throwable  {
 Class<Double> class0 = Double.class;
 String string0 = ClassUtils.getPackageName(class0);
 try { 
   ClassUtils.getClass(string0);
   fail("Expecting exception: ClassNotFoundException"); 
 } catch(ClassNotFoundException e) {
   assertThrownBy("java.net.URLClassLoader", e);
 }
}
\end{lstlisting}

While compiling and executing this test with JUnit works without
problems, the mutation analysis step of the competition used Ant to
run the tests; Ant uses a complex setup of classloaders that
eventually leads to the \texttt{assertThrownBy} in the above example
to fail, as the source of the exception is a different one.

There are 15 runs in total where \EVOSUITE did not produce any test
suites; these are only for higher time budgets (240s, 480s). The
majority of these runs are due to \EVOSUITE not terminating before the
hard timeout of the competition infrastructure. This may happen, for
example, when test execution on the class under test takes long (e.g.,
timeouts), and when resetting the static state of the classes under
test takes a long time. However, this number is still lower than the
number of erroneous runs compared to other tools, and in the remaining
1617 runs of the competition \EVOSUITE terminated in time and produced
at least two tests. In 13 of these, \EVOSUITE produced a test suite
with a compilation error.

%------------------------------------------------------------------------- 
\section{Conclusions}

With an overall score of 1126.7, \EVOSUITE achieved the highest score
of all tools in the competition. 


To learn more about \EVOSUITE, visit our Web site:
\begin{center}
%\url{http://evosuite.org/}
\texttt{http://www.evosuite.org}
\end{center}


%------------------------------------------------------------------------- 

%\noindent
\textbf{Acknowledgments:} Many thanks to all the contributors to \EVOSUITE.
This project has been funded by 
%the EPSRC project ``EXOGEN'' (EP/K030353/1), a Google Focused Research Award on ``Test
%Amplification'', and by 
the National Research Fund, Luxembourg (FNR/P10/03).


%------------------------------------------------------------------------- 
%\def\IEEEbibitemsep{5pt plus 1pt}
%\def\IEEEbibitemsep{6pt}

\bibliographystyle{IEEEtranS}
\bibliography{papers}
\balance

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
