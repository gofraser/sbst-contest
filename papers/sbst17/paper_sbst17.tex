%--------1---------2---------3---------4---------5---------6---------7
%
% Competition paper
% SBST 2013
% Page limit: 4 pages
%

%\documentclass[10pt, conference, compsocconf]{IEEEtran}
%\documentclass[10pt,conference,compsocconf]{IEEEtran}
\documentclass[10pt,conference]{IEEEtran} 


%\documentclass[times, 10pt,twocolumn]{article} 
%\usepackage{latex8}
%\usepackage{times}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage[english]{babel}
\usepackage{listings}
\usepackage{algorithmic}
\usepackage{float}
\usepackage[numbers,sort&compress,square]{natbib}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{subfigure}
%\usepackage{hyperref}
\usepackage{color}
\usepackage[usenames,dvipsnames,table]{xcolor}
\usepackage{soul}
\usepackage{xspace}
\usepackage{boxedminipage}
\usepackage{alltt}
\usepackage{multirow}
\usepackage{paralist}
\usepackage{amsmath}
\usepackage{balance}
\definecolor{light-gray}{gray}{0.90}

\floatstyle{ruled}
\newfloat{algorithm}{tbp}{loa}
\floatname{algorithm}{Algorithm}

\newtheorem{definition}{Definition}


 %krams hinter fontadjust ist neu
  \definecolor{lightgrey}{rgb}{0.90,0.90,0.90}
\lstset{escapeinside={(*}{*)}}
  \lstloadlanguages{java}
 \lstdefinelanguage{pseudocode}
  {morekeywords={if, else, initialize, return, for, each, in, global, new}
   }
  \lstset{
    tabsize=2,
    mathescape=true,
    escapeinside={(*}{*)},
    captionpos=t,
    framerule=0pt,
    backgroundcolor=\color{lightgrey},
    basicstyle=\scriptsize\ttfamily,
    keywordstyle=\footnotesize\bfseries,
    numbers=none,
    numberstyle=\tiny,
    numbersep=1pt,
    fontadjust,
    breaklines=true,
    breakatwhitespace=false
  }    
      

% \hypersetup{
% colorlinks=true,
% urlcolor=rltblue,
% linkcolor=rltred,
% citecolor=rltgreen,
% bookmarksnumbered=true,
% pdftitle={EvoSuite at the SBST 2016 Tool Competition},
% pdfauthor={Gordon Fraser and Andrea Arcuri},
% pdfsubject={Test case generation},
% pdfkeywords={Test case generation, unit testing, test
%   oracles, assertions, search based testing}
% }

\definecolor{rltred}{rgb}{0.5,0,0}
\definecolor{rltgreen}{rgb}{0,0.5,0}
\definecolor{rltblue}{rgb}{0,0,0.5}
\definecolor{ScarletRed}{rgb}{0.80,0.00,0.00}



% in draft mode we put \remarks into the margins and do other stuff
% set to \draftfalse for 
\newif\ifdraft
\draftfalse

\ifdraft
	\marginparwidth=1.3cm
	\marginparsep=5pt
	\newcommand\remark[1]{%
		\mymarginpar{\raggedright\hbadness=10000\tiny\it #1\par}}
\else
	\newcommand\remark[1]	{}
\fi

\ifdraft
	\overfullrule3pt
\fi    

% We use \FIXME for located problems (``defect'')
\newcommand{\FIXME}[1]{\remark{FIXME: #1}}
\newcommand\parremark[1]	{\par\textbf{REMARK:} #1\par}

\newcommand{\gordon}[1]{\textcolor{blue}{\sf\small\textbf{Gordon:} #1}}
\newcommand{\andrea}[1]{\textcolor{ScarletRed}{\sf\small\textbf{Andrea:} #1}}

% \mathid is used to denote identifiers and slots in formulas
\newcommand{\mathid}[1]{\text{\rmfamily\textit{#1}}}

% But usually, we shall use \|name| instead.
\def\|#1|{\mathid{#1}}

% \codeid is used to denote computer code identifiers
\newcommand{\codeid}[1]{\texttt{#1}}

% But usually, we shall use \<name> instead.
\def\<#1>{\codeid{#1}}

% Our results
\newenvironment{result}%
{\smallskip
\noindent
\let\emph=\textbf
\begin{boxedminipage}{\columnwidth}\begin{center}\em}%
{\end{center}\end{boxedminipage}%
\smallskip
}

\newcommand{\JodaTime}{Joda-Time\xspace}  % That's how they write themselves -- AZ

\newcommand{\EVOSUITE}{{\sc EvoSuite}\xspace}
\newcommand{\MUTEST}{{\sc $\mu$Test}\xspace}
\newcommand{\CS}{{\sc SF100}\xspace}

% TODO marker
\newcommand{\TODO}[1]{\sethlcolor{yellow}\textbf{\textcolor{ScarletRed}{\hl{TODO: #1}}}\xspace}


\DeclareMathSymbol{,}{\mathpunct}{letters}{"3B}
\DeclareMathSymbol{,}{\mathord}{letters}{"3B}
\DeclareMathSymbol{\decimal}{\mathord}{letters}{"3A}
%%%"

\input{macros}

%------------------------------------------------------------------------- 
% take the % away on next line to produce the final camera-ready version 
%\pagestyle{empty}

%------------------------------------------------------------------------- 
\begin{document}

% 

%\title{Unit Testing Tool competition: Results for EvoSuite}
\title{EvoSuite at the SBST 2017 Tool Competition}
 

\author{\IEEEauthorblockN{Gordon Fraser\IEEEauthorrefmark{1}, Jos\'e
    Miguel Rojas\IEEEauthorrefmark{2}, Jos\'e Campos\IEEEauthorrefmark{3}}
  \IEEEauthorblockA{The University of Sheffield\\Sheffield, United Kingdom\\
    \{\IEEEauthorrefmark{1}gordon.fraser,
    \IEEEauthorrefmark{2}j.rojas, \IEEEauthorrefmark{3}jose.campos\}@sheffield.ac.uk} 
\and
\IEEEauthorblockN{Andrea Arcuri}
  \IEEEauthorblockA{Westerdals Oslo ACT, Norway\\and University of Luxembourg, Luxembourg\\arcand@westerdals.no}
}

\maketitle
%\thispagestyle{empty}

\begin{abstract}
  \EVOSUITE is a search-based tool that automatically generates unit
  tests for Java code.  This paper summarizes the results and
  experiences of \EVOSUITE's participation at the fifth unit testing
  competition at SBST 2017, where \EVOSUITE achieved the highest
  overall score.
\end{abstract}



%------------------------------------------------------------------------- 
\section{Introduction}
The annual unit test generation competition aims to drive and evaluate
progress on unit test generation tools. In the 5th instance of the
competition at the International Workshop on Search-Based Software
Testing (SBST) 2017, four tools competed on a set of open source Java
classes. This paper describes the results of applying the \EVOSUITE
test generation tool~\cite{FrA11c} in this competition. Details about
the procedure of the competition, the technical framework, and the
benchmark classes can be found in~\cite{sbst17competition}.  In this
competition, \EVOSUITE achieved a 1457.3 overall score, which was the
highest among the competing and baseline tools.

%------------------------------------------------------------------------- 
\section{About EvoSuite}


\begin{table}[!h]
\renewcommand{\arraystretch}{1.3}
\caption{Classification of the \EVOSUITE unit test generation
  tool.}\label{tool-description}
\resizebox{1.0\columnwidth}{!}{  
\begin{tabular}{|l|p{5cm}|}
  \hline
  \multicolumn{2}{|l|}{Prerequisites} \\
  \hline
  Static or dynamic &  Dynamic testing at the Java class level\\
  Software Type &  Java classes\\
  Lifecycle phase&  Unit testing for Java programs\\
  Environment&  All Java development environments \\
  Knowledge required & JUnit unit testing for Java\\
  Experience required &  Basic unit testing knowledge\\
 \hline
  \multicolumn{2}{|l|}{Input and Output of the tool} \\
  \hline
 Input & Bytecode of the target class and dependencies \\
\hline
Output&  JUnit 4 test cases\\
 
  \hline
  \multicolumn{2}{|l|}{Operation} \\
  \hline
  Interaction &  Through the command line, and plugins for IntelliJ, Maven and Eclipse\\
  User guidance &  manual verification of assertions for functional faults\\
  Source of information &  http://www.evosuite.org \\
  Maturity&  Mature research prototype, under development\\
  Technology behind the tool & Search-based testing / whole test suite generation\\
\hline
  \multicolumn{2}{|l|}{Obtaining the tool and information} \\
  \hline
License & Lesser GPL V.3\\
Cost & Open source\\
Support & None \\
\hline
\hline
  \multicolumn{2}{|l|}{Does there exist empirical evidence about} \\
  \hline
  Effectiveness and Scalability & See~\cite{GoA_TSE12,fraser2014large}. \\
%Completeness & \\
%Effectiveness & \\
%Efficiency & \\
%Defect types & \\
%Scalability & \\
%Comprehensibility & \\
%Learnability & \\
%Subjective satisfaction & \\
%Other & \\
\hline
\end{tabular}\vspace{-1em}
}
\end{table}


\EVOSUITE~\cite{FrA11c} is a search-based tool~\cite{GoA_TSE12} that
uses a genetic algorithm to automatically generate test suites for
Java classes. Given the name of a target class and the full Java
classpath (i.e., where to find the compiled bytecode of the class
under test and all its dependencies), \EVOSUITE automatically produces
a set of JUnit test cases that maximise the achieved code
coverage. \EVOSUITE can be used on the command line, or through
plugins for popular development tools such as IntelliJ, Eclipse, or Maven~\cite{ICST16_Tool}.

The underlying genetic algorithm uses test suites as representation
(chromosomes). Each test suite consists of a variable number of test
cases, each of which is represented as a variable length sequence of
Java statements (e.g., calls on the class under test). A population of
randomly generated individuals is evolved using suitable search
operators (e.g., selection, crossover and mutation), such that
iteratively better solutions with respect to the optimisation target
are produced. The optimisation target is to maximise code coverage. To
achieve this, the fitness function uses standard heuristics such as
the branch distance; see~\cite{GoA_TSE12} for more details. \EVOSUITE
can be configured to optimise for multiple coverage criteria at the
same time, and the default configuration combines branch coverage with
mutation testing~\cite{emse14_mutation} and other basic
criteria~\cite{rojas2015combining}. Once the search is completed,
\EVOSUITE applies various optimisations to improve the readability of
the generated tests. For example, tests are minimised, and a
minimised set of effective test assertions is selected using mutation
analysis~\cite{10.1109/TSE.2011.93}. For more details on the tool and
its abilities we refer to~\cite{FrA11c,FrA13a}.


The effectiveness of \EVOSUITE has been evaluated on open source as
well as industrial software in terms of code
coverage~\cite{fraser2014large,emse_archive}, fault finding
effectiveness~\cite{shamshiri2015automatically,moein2017}, and effects
on developer productivity~\cite{TOSEM_userstudy,ISSTA15_Study}.

In the first two and the fourth editions of the unit testing tool
competition, \EVOSUITE ranked
first~\cite{evosuiteAtSbst2013,evosuiteAtFittest2013,evosuiteAtSbst2016},
whereas it ranked second in the third one.






%To improve performance further, we are investigating several
%extensions to \EVOSUITE. 
%For example, \EVOSUITE can employ dynamic
%symbolic execution~\cite{evoISSRE113} 
%and memetic algorithms~\cite{fraser2014memetic}
%to handle the cases in which our genetic algorithm may struggle. 


%------------------------------------------------------------------------- 
\section{Competition Setup}

The configuration of \EVOSUITE for the competition is largely based on
its default values, since these have been tuned
extensively~\cite{arcuri2013parameter}. We used the default set of
coverage criteria~\cite{rojas2015combining} (e.g., line coverage,
branch coverage, branch coverage by direct method invocation, weak
mutation testing, output coverage, exception coverage). The use of an
archive of solutions~\cite{emse_archive}, which iteratively removes
covered goals from the fitness function and stores the corresponding
test cases is now enabled by default in \EVOSUITE.

A new feature in \EVOSUITE is the use of Mockito mock
classes~\cite{ICST_Mocking17}. After a certain percentage of the
search budget has passed, \EVOSUITE starts considering the use of mock
objects instead of real classes. Only branches that cannot be covered
without mocks will result in tests with mock objects in the end. We
further added frequency based weighting to constants for
seeding~\cite{sakti2015instance}, and included extensions to support
Java Enterprise Edition features~\cite{arcuri2016java}. Besides these
changes, several bug fixes were applied since the last instance of the
competition, in particular in relation to non-determinism and flaky
tests~\cite{arcuri2014automated}.

Like in previous instances of the competition, we enabled the
post-processing step of test minimisation (but not for efficiency
reasons, but because minimised tests are less likely to break). To
reduce the overall time of test generation we included all assertions
rather than filtering them with mutation
analysis~\cite{10.1109/TSE.2011.93}, which is a computationally
expensive process. The use of all assertions has a negative impact on
readability, but this is not evaluated as part of the SBST contest.

Like in the 2016 competition, tools were called with different time
budgets. We used the same strategy as for the previous
competition~\cite{evosuiteAtSbst2016} to distribute the overall time
budget onto the different phases of \EVOSUITE (e.g., initialization,
search, minimization, assertion generation, compilation check, removal
of flaky tests). That is, 50\% of the time was allocated to the
search, and the rest was distributed equally to the remaining
phases.


%------------------------------------------------------------------------- 
\section{Benchmark results}

\begin{table*}[t]
  \centering
  \caption{\label{table:results}Detailed results of \EVOSUITE on the SBST benchmark classes.}
\resizebox{\textwidth}{!}{  
\input{mainTable.tex}
}	
\end{table*}

% \begin{table*}[t]
%   \centering
%   \caption{\label{table:coverage_results}Detailed coverage results of \EVOSUITE on the SBST benchmark classes.}
% \resizebox{0.8\textwidth}{!}{  
% \input{coverageTable.tex}
% }
% \end{table*}

% \begin{table*}[t]
%   \centering
%   \caption{\label{table:fault_results}Detailed fault detection results of \EVOSUITE on the SBST benchmark classes.}
% \resizebox{0.8\textwidth}{!}{  
% \input{faultTable.tex}
% }	
% \end{table*}

\subsection{Overall results}

Table~\ref{table:results} lists the branch coverage and mutation
analysis results achieved by \EVOSUITE on all benchmark classes in the
contest. Coverage is generally in the expected range, with clear
overall increases for higher time budgets. With $budget=480s$, the
average branch coverage achieved was \AvgCovG (slightly higher than
last year's $65.6\%$) and the average mutation score was \AvgMutG
(considerably higher than last year's $41.0\%$).

This year, the contest also included manually written test suites as
baseline. \EVOSUITE significantly outperformed this manual baseline as
well, both in terms of branch coverage (avg. \AvgCovEvosuite vs
\AvgCovManual, with Vargha-Delaney's $A_{12}=\EvoManCovA$ and
$p\EvoManCovPV$) and mutation scores (avg. \AvgMutEvosuite vs
\AvgMutManual, with $A_{12}=\EvoManMutA$ and
$p\EvoManMutPV$).\TODO{Does showing A12 make sense if there's only one
  manual test suite per benchmark?}

% Coverage on several
% of the benchmarks from Closure is low, which matches previous
% findings~\cite{shamshiri2015automatically}.

% Results in terms of mutation scores and fault detection ratio (i.e.,
% how many of the runs had at least one failing test on the
% corresponding bug) are showing in
% Table~\ref{table:fault_results}. Again the Closure results are
% generally worse than those of other projects, both in terms of
% mutation score and ratio of fault detection.

\EVOSUITE generated \FlakyEvosuite flaky tests per run on average,
much lower than the number of flaky tests produced by the competing
and baseline tools (\FlakyTthree by T3, \FlakyJtexpert by JTExpert, and
\FlakyRandoop by Randoop). We attribute these results---also
consistent with previous years' results---to the way \EVOSUITE handles
execution environments during test generation (e.g., controlling the
static state of the class under test, mocking interactions with the
file system, system calls like \texttt{System.in} and
\texttt{System.currentTimeMillis}, etc.)~\cite{arcuri2014automated}.


\subsection{Challenges}

% Look at benchmarks that always failed
\EVOSUITE failed to produce any test suite for benchmarks JXPATH-7 and
OKHTTP-8, and also struggled often for benchmarks LA4J-3, LA4J-7,
BCEL-9 (highlighted in Table~\ref{table:results}). All executions of
\EVOSUITE for JXPATH-7 failed in the instrumentation phase as a result
of exceeding Java's 64k limit on the size of methods. A viable fix
would be to stop instrumenting before the limit is reached, at the
price of limited search guidance; a more effective solution would
involve identifying parts of the code that are worth instrumenting. A
missing dependency for OKHTTP-8 caused all executions---\emph{for all
  tools}---to fail. For the other mentioned benchmarks, the failure
reasons (in some cases related to sandboxing, mocking and timeouts)
will require further investigation.

% Look at benchmarks where coverage decreased with increased budget
As expected, both branch coverage and mutation score generally
increased with higher time budgets, although in some cases, specially
at $budget=300s$, a decrease was observed for some benchmarks. Having
run only three repetitions of the tool per time budget, it is fair to
assume the decreases are due to bugs affecting executions with
$budget=300s$ by chance. A preliminary investigation on the source of
this loss of coverage revealed that out-of-memory errors (e.g., for
OKHTTP-2 and RE2J-7), sandboxing problems (e.g., for LA4J-3 and
LA4J-7), and mocking-related issues (e.g., for BCEL-1, BCEL-5, BCEL-7)
are some of the problems in \EVOSUITE.

% Although coverage generally increased with higher time budgets, there
% are instances where this expected behaviour was not observed,
% specially for $budget=300s$, where the average across benchmarks
% decreased. This is possibly due to crashes or out-of-memory runtime
% errors. For instance, \EVOSUITE failed to generate any test suite in
% one out of three runs of OKHTTP-2 and RE2J-7 due to an
% \textit{out-of-memory error}, and in all executions of LA4J-3 and
% LA4J-7 due to a \textit{state exception}. In order to evaluate a
% fitness function, each test case evolved by the underline GA of
% \EVOSUITE has to be executed. However, its execution could fail due to
% several reasons: state exception (if a test case tries to re-initalize
% EvoSuite's sanbox, LA4J-3 and LA4J-7), timeout (OKHTTP-2), etc. In
% case of a timeout, \EVOSUITE logs the stacktrace of the thread in
% which each test case is executed, which could cause an out-of-memory
% error if the stack trace message is to long. RE2J-7: \TODO{I do not
% understand how a java.lang.OutOfMemoryError could be throw by
% CBranchSuiteFitness.java:145}.
% %
% To avoid low coverage of final classes with private constructors (like
% the class under test of benchmark OKHTTP-7), \EVOSUITE uses reflection
% to call the default constructor of the class under test. However,
% \TODO{EvoSuite tries to access a field of the cut (which is public
% static final) by using an instance of cut. It's still not clear why}.
% For other benchmarks such for example BCEL-1, BCEL-5, BCEL-7,
% \EVOSUITE failed due to mocking related issues.

% Look at benchmarks with more flaky tests
Flaky tests continue to be a challenge for test generation. Benchmarks
FREEHEP-7 and JXPATH-10 were the ones with the highest number of flaky
tests in one run (12 and 14, respectively). In both cases, the
minimisation phase timed out and hence \EVOSUITE reverted the
resulting test suite to the previous, unminimised one, which as
mentioned before, is more likely to break. Techniques such as
\textit{partial minimisation} to only select a few test cases to be
minimised, rather than all test cases; or
\textit{delta-debugging}~\cite{zellerdd}, which could isolate the
relevant statements of each test case, might be worth investigating
to alleviate this issue.

% Mocking
As mentioned in the previous section, \EVOSUITE now uses private
API access and functional mocking%  in addition to environment
% mocking
~\cite{ICST_Mocking17}. In the competition, a total of 20,921
objects were mocked, 1,485 private fields were set and 3,090 private methods
were invoked using mocked access% , and environment classes were
% mocked 4,699 times, with \texttt{java.util.Random} and
% \texttt{java.io.File} being by far the most frequently mocked
% environment classes (resp. 1,286 and 1,121 times)
, adding up to represent only 1.09\% of the total number of lines of code
in the \EVOSUITE-generated test suites (2,329,361 LOC). Further
investigation is needed to assess the impact of these mocking features
on test generation effectiveness for the benchmarks in the
competition.




% A few of these flaky tests were introduced by recent changes to
% \EVOSUITE following experiments on
% Defects4J~\cite{shamshiri2015automatically}: \EVOSUITE now includes
% assertions on the source of exceptions, similar to commercial tools
% like Agitar One. %~\cite{agitarone}.
% Unfortunately, there were several instances in the competition where
% these assertions lead to flaky tests. For example, the following is an
% excerpt from a test for the Defects4J bug Lang-41, generated by
% \EVOSUITE: \vspace{1em}

% \begin{lstlisting}
% @Test(timeout = 4000)
% public void test19()  throws Throwable  {
%  Class<Double> class0 = Double.class;
%  String string0 = ClassUtils.getPackageName(class0);
%  try { 
%    ClassUtils.getClass(string0);
%    fail("Expecting exception: ClassNotFoundException"); 
%  } catch(ClassNotFoundException e) {
%    assertThrownBy("java.net.URLClassLoader", e);
%  }
% }
% \end{lstlisting}

% While compiling and executing this test with JUnit works without
% problems, the mutation analysis step of the competition used Ant to
% run the tests; Ant uses a complex setup of classloaders that
% eventually leads to the \texttt{assertThrownBy} in the above example
% to fail, as the source of the exception is a different one.

% There are 15 runs in total where \EVOSUITE did not produce any test
% suites; these are only for higher time budgets (240s, 480s). The
% majority of these runs are due to \EVOSUITE not terminating before the
% hard timeout of the competition infrastructure. This may happen, for
% example, when test execution on the class under test takes long (e.g.,
% timeouts), and when resetting the static state of the classes under
% test takes a long time. However, this number is still lower than the
% number of erroneous runs compared to other tools, and in the remaining
% 1617 runs of the competition \EVOSUITE terminated in time and produced
% at least two tests. In 13 of these, \EVOSUITE produced a test suite
% with a compilation error.

%------------------------------------------------------------------------- 
\section{Conclusions}

With an overall score of 1126.7, \EVOSUITE achieved the highest score
of all tools in the competition. 


To learn more about \EVOSUITE, visit our Web site:
\begin{center}
%\url{http://evosuite.org/}
\texttt{http://www.evosuite.org}
\end{center}


%------------------------------------------------------------------------- 

%\noindent
\textbf{Acknowledgments:} Many thanks to all the contributors to \EVOSUITE.
This project has been funded by the EPSRC
project ``GREATEST'' (EP/N023978/1), and by the National Research
Fund, Luxembourg (FNR/P10/03).


%------------------------------------------------------------------------- 
%\def\IEEEbibitemsep{5pt plus 1pt}
%\def\IEEEbibitemsep{6pt}
%\clearpage
\bibliographystyle{IEEEtranS}
\bibliography{papers}
\balance

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
